import pandas as pd
import numpy as np
import csv
import os
import logging
from io import StringIO
from datetime import datetime

# === Configuration du logging ===
logging.getLogger(__name__)

local_xlsx_path = './Staging/data/sa_insern_n_2_n_1.xlsx'
local_csv_path = './Staging/data/sa_insern_n_2_n_1.csv'


# === Fonctions principales ===
def fill(df: pd.DataFrame) -> pd.DataFrame:
    """
    Fonction de remplissage des valeurs manquantes de la premi√®re colonne selon la m√©thode ffill.

    Parameters
    ----------
    df : pd.DataFrame
        Dataframe (TCD) sur lequel appliquer le remplissage.

    Returns
    -------
    pd.DataFrame
        Dataframe avec la premi√®re colonne remplie (sans valeur manquante).
    """
    # Remplir les valeurs manquantes dans la colonne "Colonne1"
    first_col = df.columns[0]
    df[first_col] = df[first_col].ffill()
    return df


def TCD_management(df: pd.DataFrame):
    for col_name in df.columns:
        # Si la colonne contient "Total g√©n√©ral" -> suppression de la ligne
        df = df[df[col_name] != "Total g√©n√©ral"]
        
        # Remplacement de "(vide)" par None
        mask = df[col_name] == "(vide)"
        df.loc[mask] = None
    return df


def convert_excel_to_csv(local_xlsx_path: str, local_csv_path: str):
    """
    Convertie un fichier Excel en csv (';').

    Parameters
    ----------
    local_xlsx_path : str
        Chemin local du fichier .xlsx.
    local_csv_path : str
        Chemin local du fichier .csv rempla√ßant le .xlsx.
    """
    # Conversion du fichier excel vers du csv
    df = pd.read_excel(local_xlsx_path, engine='openpyxl')
    df = fill(df)
    df = TCD_management(df)
    df.index.names = ['Column1']
    df.to_csv(local_csv_path, sep=";", quoting=csv.QUOTE_NONNUMERIC)

    # Suppression du fichier Excel
    try:
        os.remove(local_xlsx_path)
        logging.info(f"Fichier remplac√© par un csv : {local_xlsx_path} -> {local_csv_path}")
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Erreur lors de la suppression de {local_xlsx_path} : {e}")

class ReadCsvWithDelimiter:
    def __init__(self, filepath, sample_size=4096):
        self.filepath = filepath
        self.sample_size = sample_size
        self.dialect = self.detect_delimiter()

    def detect_delimiter(self):
        """D√©tecte automatiquement le d√©limiteur du fichier CSV."""
        try:
            with open(self.filepath, 'r', encoding='utf-8-sig') as f:
                sample = f.read(self.sample_size)
                sniffer = csv.Sniffer()
                dialect = sniffer.sniff(sample, delimiters=";,¬§")
                return dialect.delimiter
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Impossible de d√©tecter le d√©limiteur pour {self.filepath} : {e} ‚Üí ';' utilis√© par d√©faut.")
            return ";"

    def read_csv_resilient(self):
        """Tente de lire un CSV avec diff√©rents d√©limiteurs."""
        delimiters_to_try = [self.dialect, ";", ",", "¬§"]
        tried = set()

        for delimiter in delimiters_to_try:
            if delimiter in tried:
                continue
            tried.add(delimiter)

            try:
                df = pd.read_csv(
                    self.filepath,
                    delimiter=delimiter,
                    dtype=str,
                    quotechar='"',
                    encoding='utf-8-sig'
                )
                logging.info(f"‚úÖ Lecture r√©ussie avec le d√©limiteur '{delimiter}' pour {os.path.basename(self.filepath)}")
                return df
            except pd.errors.ParserError as e:
                logging.warning(f"‚ö†Ô∏è Erreur de parsing avec '{delimiter}' pour {self.filepath} ‚Üí {e}")
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Autre erreur avec '{delimiter}' ‚Üí {e}")

        raise ValueError(f"‚ùå Impossible de lire le fichier CSV {self.filepath} avec les d√©limiteurs connus.")

    def read_csv_with_custom_delimiter(self):
        try:
            with open(self.filepath, "rb") as f:
                raw = f.read()

            decoded = raw.decode("utf-8", errors="replace")

            df = pd.read_csv(
                StringIO(decoded),
                delimiter="¬§",
                dtype=str,
                engine="python",
                quoting=csv.QUOTE_NONE,
                on_bad_lines="warn"
            )
            
            logging.info(f"‚úÖ Lecture r√©ussie avec d√©limiteur '¬§' apr√®s d√©tection binaire : {os.path.basename(self.filepath)}")
            return df
        except Exception as e:
            logging.error(f"‚ùå Erreur lors de la lecture de {self.filepath} avec d√©limiteur '¬§' ‚Üí {e}")
            raise

    def read_csv_files(self)-> pd.DataFrame:
        try:
            if self.filepath.name == "sa_sivss.csv":
                return self.read_csv_with_custom_delimiter()
            else:
                return self.read_csv_resilient()
        except Exception as e:
            logging.error(f"‚ùå Lecture √©chou√©e pour {self.filepath.name} ‚Üí {e}")
            return

def check_missing_columns(csv_file_name: str, df: pd.DataFrame, schema_df):
    table_columns = schema_df["column_name"].tolist()
    csv_columns = df.columns.tolist()

    missing_columns = set(table_columns) - set(csv_columns)
    extra_columns = set(csv_columns) - set(table_columns)

    if missing_columns:
        logging.warning(f"Colonnes manquantes dans {csv_file_name} : {missing_columns}")
        for col in missing_columns:
            df[col] = None

    if extra_columns:
        logging.warning(f"Colonnes en trop dans {csv_file_name} : {extra_columns}")
        df = df[table_columns]

def convert_columns_type(type_mapping: dict, df: pd.DataFrame, schema_df)-> pd.DataFrame:
    for _, row in schema_df.iterrows():
        col_name = row["column_name"]
        col_type = str(row["column_type"])

        if col_name in df.columns and col_type in type_mapping:
            try:
                if type_mapping[col_type] in ["int", "float"]:
                    df[col_name] = df[col_name].replace({None: 0, "": 0, pd.NA: 0, "nan": 0}).astype(float).astype(type_mapping[col_type])
                elif type_mapping[col_type] == "bool":
                    df[col_name] = df[col_name].replace({None: False, "": False, pd.NA: False}).astype(bool)
                elif type_mapping[col_type] == "datetime64":
                    df[col_name] = pd.to_datetime(df[col_name], format="%d-%m-%Y", errors="coerce")
                else:
                    df[col_name] = df[col_name].astype(type_mapping[col_type])
            except ValueError as e:
                logging.warning(f"Erreur de conversion de {col_name} en {col_type}: {e}, valeurs laiss√©es en str.")
    return df

def export_to_csv(table_name: str, csv_name: str, df_fetch_func, output_folder: str, date: datetime):
    """ Export une table de DuckDB vers un csv"""
    os.makedirs(output_folder, exist_ok=True)

    # date = date.strftime(date.today(), "%Y_%m_%d") 
    file_name = f'sa_{csv_name}_{date}.csv'
    output_path = os.path.join(output_folder, file_name)
    logging.info(f"üì§ Export de '{table_name}' ‚Üí {output_path}")

    try:
        df = df_fetch_func(table_name)
        df.to_csv(output_path, index=False, sep=";", encoding="utf-8-sig")
        logging.info(f"‚úÖ Export r√©ussi : {file_name}")
    except Exception as e:
        logging.error(f"‚ùå Erreur d'export pour '{table_name}' ‚Üí {e}")

if __name__ == "__main__":
    convert_excel_to_csv(local_xlsx_path, local_csv_path)
